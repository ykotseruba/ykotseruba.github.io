<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Iuliia Kotseruba MSc</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css"/>
		<link rel="stylesheet" href="assets/css/academicons.css"/>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a class="image avatar"><img src="images/avatar.jpg" alt="" /></a>
					<h1><strong>Iuliia Kotseruba</strong><br /> 
					PhD Candidate at Tsotsos Lab<br />
					for <a href="http://jtl.lassonde.yorku.ca/">Active and Attentive Vision</a>.</h1>
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="one">
<!-- 						<header class="major">
							<h2>Ipsum lorem dolor aliquam ante commodo<br />
							magna sed accumsan arcu neque.</h2>
						</header>
						<p>
 -->
						<ul class="actions">
							<li><a href="#recent_work" class="button">Recent Work</a></li>
							<li><a href="#publications" class="button">Publications</a></li>
							<li><a href="#datasets" class="button">Datasets</a></li>
							<li><a href="#cv" class="button">CV</a></li>
						</ul>

						I am a PhD student supervised by Prof. John K. Tsotsos and member of the Lab for Active and Attentive Vision at York University. </p>

						<p>I study human visual attention and its applications in saliency and human gaze prediction. One aspect of this is integrating attention and vision with other cognitive abilities in AI systems, e.g. understanding driver-pedestrian interaction for safer autonomous driving. In addition, I work on analyzing gaps in human-machine performance in saliency and object detection.
						</p>


					</section>

				<!-- Two -->
					<section id="recent_work">
						<h2>Recent Work</h2>
						<div class="row">
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/behavioral_topics_extended.png" class="image fit thumb"><img src="images/thumbs/behavioral_topics_extended.png" alt="" /></a>
								<h3>Driving and Attention</h3>
								<p>A report and curated database of >400 papers (since 2010) on all aspects of attention during driving. We focus on studies where human gaze was recorded and analyzed. We first give an overview of human gaze, pros and cons of using it as a proxy for attention, and procedures for recording data. We then review behavioral research on drivers' attention and identified multiple factors, external and internal, that affect gaze allocation. The second half of the report is dedicated to analytical models of attention and various practical solutions that rely on drivers' gaze.  
								<br>
								[<a href="https://arxiv.org/pdf/2104.05677.pdf">Report</a>][<a href="https://github.com/ykotseruba/attention_and_driving">Database</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/jaad.jpg" class="image fit thumb"><img src="images/thumbs/crossing_prediction.png" alt="" /></a>
								<h3>Pedestrian Action Benchmark</h3>
								<p>This is the first benchmark of the pedestrian action prediction algorithms that ranks a number of baselines and state-of-the-art approaches using two public datasets for studying pedestrian behavior in traffic: <a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/">JAAD</a> and <a href="http://data.nvision2.eecs.yorku.ca/PIE_dataset/">PIE</a>. We analyze the performance of the evaluated models with respect to various properties of the data and based on the analysis propose a new model, PCPA, that combines explicit and implicit features via temporal and modality attention mechanisms and demonstrates state-of-the-art results.
								<br>
								[<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kotseruba_Benchmark_for_Evaluating_Pedestrian_Action_Prediction_WACV_2021_paper.pdf">Paper</a>][<a href="https://github.com/ykotseruba/PedestrianActionBenchmark">Code</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/sfgru.jpg" class="image fit thumb"><img src="images/thumbs/PIE.png" alt="" /></a>
								<h3>Pedestrian Intention Estimation</h3>
								<p>Pedestrian intention is an early predictor of their future action. For example, pedestrian may be standing at the intersection waiting for the safe moment to cross or may be engaged in conversation or waiting for a cab. To study these behaviors, we collected a publicly available dataset with a variety of traffic scenarios and conducted a large-scale human experiment to gather human reference data for pedestrian intention. We also show that trajectory estimation can be improved by adding pedestrian intention to other context features.<br>
				               [<a href="http://data.nvision2.eecs.yorku.ca/PIE_dataset/">PIE Dataset</a>][<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf">Paper</a>][<a href="https://github.com/aras62/PIEPredict">Code</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/jaad.jpg" class="image fit thumb"><img src="images/thumbs/jaad.jpg" alt="" /></a>
								<h3>Driver-Pedestrian Interaction</h3>
								<p>Before safely deploying autonomous vehicles many challenges still have to be resolved. One of them is the interaction with vulnerable road users such as pedestrians, particularly at the point of crossing. We collected a large-scale naturalistic dataset and used it to conduct a number of studies to investigate how human drivers interact and resolve potential road conflicts with pedestrians.<br>
								[<a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/">JAAD Dataset</a>][<a href="https://arxiv.org/abs/1609.04741">Paper</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/sfgru.jpg" class="image fit thumb"><img src="images/thumbs/sfgru.jpg" alt="" /></a>
								<h3>Pedestrian Action Anticipation</h3>
								<p>We propose a novel stacked RNN architecture SF-GRU for pedestrian crossing action prediction. In this network multimodal sources of information are gradually fused at different levels of processing. We show how length of observation, time to event, type and order of fusion affects the performance of the model.<br>
				               [<a href="https://bmvc2019.org/wp-content/uploads/papers/0283-paper.pdf">Paper</a>][<a href="https://github.com/aras62/SF-GRU">Code</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/cog_arch.jpg" class="image fit thumb"><img src="images/thumbs/cog_arch.jpg" alt="" /></a>
								<h3>40 Years of Cognitive Architectures</h3>
								<p>We summarized and catalogued various approaches to cognitive architecture design from multiple disciplines spanning areas from engineering to neuroscience. This resulted in a large-scale survey of 84 cognitive architectures developed in the past 40 years. We also assessed the practical viability of these approaches by aggregating information on over 900 practical applications that were implemented using the cognitive architectures in our list. <br>
				               [<a href="http://jtl.lassonde.yorku.ca/project/cognitive_architectures_survey/index.html">Project page</a>][<a href="https://link.springer.com/content/pdf/10.1007%2Fs10462-018-9646-y.pdf">Paper</a>][<a href="https://github.com/ykotseruba/cog_arch_data">Data</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/p3o3.jpg" class="image fit thumb"><img src="images/thumbs/p3o3.jpg" alt="" /></a>
								<h3>Do Saliency Models Detect Odd-One-Out Targets?</h3>
								<p>We evaluate the behavior of 20 saliency models on two new datasets: synthetic psychophysical images (P<sup>3</sup>) and natural odd-one-out images (O<sup>3</sup>). We show that majority of the models cannot discriminate targets that differ by color, orientation and size, which are the features that strongly guide human attention.<br>
               [<a href="http://data.nvision2.eecs.yorku.ca/P3O3/">P<sup>3</sup> and O<sup>3</sup> datasets</a>][<a href="https://bmvc2019.org/wp-content/uploads/papers/0203-paper.pdf">Paper</a>][<a href="https://bmvc2019.org/wp-content/uploads/papers/0203-supplementary.zip">Supplementary Material</a>][<a href="https://github.com/ykotseruba/P3O3_metrics">Code</a>]
								</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/Yarbus.gif" class="image fit thumb"><img src="images/thumbs/Yarbus.gif" alt="" /></a>
								<h3>Saccade Generator for Static Images</h3>
								<p>Modeling human fixations on images is a large subfield of visual attention. The focus of many computational saliency models is on generating saliency maps which highlight areas that are more likely to attract human attention. However, most practical applications require a sequence of fixations rather than areas of interest. To address this issue, we developed a flexible and customizable framework for direct saccade sequence generation.<br>
				               [<a href="https://github.com/TsotsosLab/STAR-FC">Code</a>][<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wloka_Active_Fixation_Control_CVPR_2018_paper.html">Paper</a>]</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/canabalt.gif" class="image fit thumb"><img src="images/thumbs/canabalt.gif" alt="" /></a>
								<h3>Video game playing</h3>
								<p>For my M.Sc. thesis I developed an algorithm to play browser video games of endless runner genre, such as Canabalt and Robot Unicorn Attack, in real time and using only visual input. The goal of this project was to implement a set of attentional mechanisms and visual processing pipeline to test the biologically-inspired concept of Cognitive Programs on a complex dynamic visual task.<br>
								[<a href="https://arxiv.org/pdf/1711.09464.pdf">Paper</a>]</p>
							</article>
						</div>
					</section>

				<!-- Three -->
					<section id="publications">
						<h2>Publications</h2>
						<p>Below is the list of select publications. Please see <a href="https://scholar.google.com/citations?user=a-UOikoAAAAJ&hl=en">my Google Scholar profile</a> for a full list.</p>
        		<h3>2021</h3>
		<li>I. Kotseruba, J. K. Tsotsos,<a href="https://arxiv.org/pdf/2104.05677.pdf"> “Behavioral Research and Practical Models of Drivers' Attention”</a>, arXiv:2104.05677, 2021.</li>

		<li>I. Kotseruba, A. Rasouli, J. K. Tsotsos,<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kotseruba_Benchmark_for_Evaluating_Pedestrian_Action_Prediction_WACV_2021_paper.pdf"> “Benchmark for Evaluating pedestrian Action Prediction”</a>, in Winter Conference on Applications of Computer Vision (WACV), 2021.</li>
				<br>
        		<h3>2020</h3>
        <li>I. Kotseruba, A. Rasouli, J. K. Tsotsos,<a href=""> “Do They Want to Cross? Understanding Pedestrian Intention for Behavior Prediction”</a>, in Intelligent Vehicles Symposium (IV), 2020.</li>
				<br>
        		<h3>2019</h3>      

        <li>A. Rasouli, I. Kotseruba, T. Kunic, J. K. Tsotsos,<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf"> “PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction”</a> International Conference on Computer Vision (ICCV), 2019 (Oral).</li>

        <li>I. Kotseruba, C. Wloka, A. Rasouli, J. K. Tsotsos,<a href="https://bmvc2019.org/wp-content/uploads/papers/0203-paper.pdf"> “Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations”</a> British Machine Vision Conference (BMVC), 2019 (Oral).</li>

        <li>A. Rasouli, I. Kotseruba, J. K. Tsotsos,<a href="https://bmvc2019.org/wp-content/uploads/papers/0283-paper.pdf"> “Pedestrian Action Anticipation usingContextual Feature Fusion in Stacked RNNs”</a> British Machine Vision Conference (BMVC), 2019.</li>

        <li>J. K. Tsotsos, I. Kotseruba, A. Andreopoulos, Y. Wu,<a href="https://arxiv.org/pdf/1908.10933"> “Why Data-Driven Beats Theory-Driven Computer Vision,”</a> International Conference on Computer Vision (ICCV) Workshops, 2019.</li>

        <li>J. K. Tsotsos, I. Kotseruba, C. Wloka,<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0224306"> “Early Salient Region Selection Does Not Drive Rapid Visual Categorization,”</a> PLOS One 14(10): e0224306, 2019.</li>
        		<br>
				<h3>2018</h3>

        <li>C. Wloka, T. Kunic, I. Kotseruba, R. Fahimi, N. Frosst, N. Bruce, J. K. Tsotsos,<a href="https://arxiv.org/pdf/1812.08848"> “SMILER: Saliency Model Implementation Library for Experimental Research,”</a> arXiv:1812.08848.</li>

				
        <li>I. Kotseruba, and J. K. Tsotsos,<a href=https://link.springer.com/content/pdf/10.1007%2Fs10462-018-9646-y.pdf">“40 Years of Cognitive Architectures: Core Cognitive Abilities and Applications,”</a> Artificial Intelligence Review, 2018.</li>

        <li>A. Rasouli, I. Kotseruba, and J. K. Tsotsos,<a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Rasouli_Its_Not_All_About_Size_On_the_Role_of_Data_ECCVW_2018_paper.pdf"> "It's Not All About Size: On the Role of Data Properties in Pedestrian Detection,"</a> in	European Conference on Computer Vision (ECCV) Workshop, pp. 210-225, 2018. </li>
				
        <li>A. Rasouli, I. Kotseruba, and J. K. Tsotsos,<a href=https://ieeexplore.ieee.org/abstract/document/8569324">“Towards Social Autonomous Vehicles: Understanding Pedestrian-Driver Interactions,”</a> in International Conference on Intelligent Transportation Systems (ITSC), pp. 729-734, 2018.</li>

        <li>C. Wloka, I. Kotseruba, J. K. Tsotsos,<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wloka_Active_Fixation_Control_CVPR_2018_paper.pdf">“Active fixation control to predict saccade sequences,”</a> in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</li>				
        
        <li>J. K. Tsotsos, I. Kotseruba, A. Rasouli, and M. D. Solbach, <a  href="https://link.springer.com/article/10.1007/s10339-018-0881-6" style="color: #cc0000"> “Visual attention and its intimate links to spatial cognition,”</a> Cognitive Processing, pp. 1–10, 2018.</li>

		<li>A. Rasouli, I. Kotseruba, and J. K. Tsotsos,<a href="https://ieeexplore.ieee.org/document/8241847/">“Understanding pedestrian behavior in complex traffic scenes,”</a> IEEE Transactions on Intelligent Vehicles, vol. 3, no. 1, pp. 61–70,2018.</li>

				<br>
				<h3>2017</h3>
		<li>A. Rasouli, I. Kotseruba, and J. K. Tsotsos,<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf">“Are they going to cross? a benchmark dataset and baseline for
			pedestrian crosswalk behavior,”</a> in International Conference on Computer Vision (ICCV) Workshop, 2017, pp. 206–213.</li>
		<li>A. Rasouli, I. Kotseruba, and J. K. Tsotsos, <a href="https://ieeexplore.ieee.org/abstract/document/7995730/">“Agreeing to cross: How drivers and pedestrians communicate,”</a> in Intelligent Vehicles Symposium (IV), 2017, pp. 264–269.</li>

				<br>
				<h3>2016</h3>
		<li>I. Kotseruba, A. Rasouli, and J. K. Tsotsos,<a href="https://arxiv.org/pdf/1609.04741.pdf"> “Joint attention in
			autonomous driving (jaad),”</a> arXiv:1609.04741, 2016.</li>
		<li>J. K. Tsotsos, I. Kotseruba, C. Wloka,<a href="https://iopscience.iop.org/article/10.1088/1742-6596/341/1/012027/pdf"> “A focus on selection for fixation,”</a> Journal of Eye Movement Research, vol. 9, no. 5, pp. 1-34, 2016.</li>
		<li>I. Kotseruba, <a href="http://yorkspace.library.yorku.ca/xmlui/bitstream/handle/10315/32339/Kotseruba_Iuliia_2016_Masters.pdf?sequence=2">“Visual Attention in Dynamic Environments and Its Application To Playing Online Games,”</a> M.Sc. thesis, York University, 2016.</li>
				<br>
				<h3>2012</h3>
		<li>I. Kotseruba, CA. Cumbaa, and I. Jurisica,<a href="https://iopscience.iop.org/article/10.1088/1742-6596/341/1/012027/pdf"> “High-throughput protein crystallization on the World Community Grid and the GPU,”</a> Journal of Physics: Conference Series, vol. 341, no. 1, pp. 12-27, 2012.</li>
	</section>

		<section id="datasets">
		<h2>Datasets</h2>
			 <li><a href="http://data.nvision2.eecs.yorku.ca/PIE_dataset/">Pedestrian Intention Estimation (PIE) Dataset</a></li>
			 <p>A collection of videos recorded in Toronto, Canada with extensive spatial and behavioral annotations to study pedestrian intention estimation and crossing behavior. </p>

			 <li><a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/">Joint Attention in Autonomous Driving (JAAD)</a></li>
			 <p>A collection of 346 clips recorded in Canada and Eastern Europe to study driver-pedestrian interaction and pedestrian behavior before crossing the street. </p>

			 <li><a href="http://data.nvision2.eecs.yorku.ca/P3O3/">Psychophysical Patterns (P<sup>3</sup>) and Odd-One-Out (O<sup>3</sup>) Datasets</a></li>
			 <p>Annotated datasets of synthetic psychophysical patterns (P<sup>3</sup>) and natural odd-one-out images (O<sup>3</sup>) for testing properties of saliency models w.r.t. features that guide human attention.</p>

		</section>
	</section>

		<section id="cv">
		<h2>CV</h2>

			 <p>Follow <a href="https://docs.google.com/document/d/1EVSry3K4Pr7tk_4SIxU_54qJBVBfgtcY7AlkM81IO_E/edit?usp=sharing">this link</a> to my CV.</p>
		</section>
			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<li><a href="https://scholar.google.com/citations?user=a-UOikoAAAAJ&hl=en" class="icon ai ai-google-scholar-square"></a></li>
						<li><a href="https://www.researchgate.net/profile/Yulia_Kotseruba" class="icon ai ai-researchgate-square"></a></li>
						<li><a href="https://github.com/ykotseruba/" class="icon brands fa fa-github"></a></li>
						<li><a href="mailto:yulia_k@eecs.yorku.ca" class="icon fa fa-envelope"></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Iuliia Kotseruba</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
