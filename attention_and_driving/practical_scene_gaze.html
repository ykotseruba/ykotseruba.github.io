<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="assets/css/tabulator/4.9/tabulator.min.css" rel="stylesheet">
		<link href="assets/css/tables.css" rel="stylesheet">
	</head>


	<body>

 <!-- Top navigation -->
<div class="topnav">

<a href="https://github.com/ykotseruba/attention_and_driving">Back to Github</a>
<a href="behavioral_studies.html">Behavioral</a>
<a href="#" class="active">Practical</a>
<a href="datasets_datasets.html">Datasets</a>
<a href="surveys_surveys.html">Surveys</a>


</div> 

<div class="topnav">

<a href="#" class="active">Scene gaze</a>
<a href="practical_in_vehicle_gaze.html">In-vehicle gaze</a>
<a href="practical_inattention_detection.html">Inattention detection</a>
<a href="practical_driver_awareness.html">Driver awareness</a>
<a href="practical_self_driving.html">Self-driving</a>
<a href="practical_factors.html">Factors</a>


</div> 

		<div id="example-table"></div>
						<script type="text/javascript" src="assets/js/tabulator/4.9/tabulator.min.js"></script>
		<script type="text/javascript">
	//sample data
	var tabledata = [
	{reference_link: "https://doi.org/10.1109/TITS.2021.3053178",reference: "<a href=\"https://doi.org/10.1109/TITS.2021.3053178\">2021_T-ITS_Lateef</a>",title: "Saliency Heat-Map as Visual Attention for Autonomous Driving Using Generative Adversarial Network (GAN)",venue: "T-ITS",year: "2021",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "VADD",metrics: "MSE, PSNR, FID, WD, KLDiv, AUCCC, NSS"},
{reference_link: "https://doi.org/10.1109/TITS.2021.3055120",reference: "<a href=\"https://doi.org/10.1109/TITS.2021.3055120\">2021_T-ITS_Amadori</a>",title: "HammerDrive: A Task-Aware Driving Visual Attention Model",venue: "T-ITS",year: "2021",code: "-",input: "RGB, EV",observation: "16 frames",output: "saliency map",dataset: "custom: sim",metrics: "KLDiv, CC, SIM, IG"},
{reference_link: "https://doi.org/10.1109/TITS.2019.2915540",reference: "<a href=\"https://doi.org/10.1109/TITS.2019.2915540\">2020_T-ITS_Deng</a>",title: "How Do Drivers Allocate Their Potential Attention? Driving Fixation Prediction via Convolutional Neural Networks",venue: "T-ITS",year: "2020",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "Deng \etal 2020",metrics: "AUC, NSS, IG, CC, SIM, EMD, KLDiv"},
{reference_link: "https://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf\">2020_CVPR_Pal</a>",title: "“Looking at the right stuff” - Guided semantic-gaze for autonomous driving",venue: "CVPR",year: "2020",code: "<a href=\"https://sites.google.com/eng.ucsd.edu/sage-net\">https://sites.google.com/eng.ucsd.edu/sage-net</a>",input: "RGB",observation: "16 frames",output: "saliency map",dataset: "DR(eye)VE; BDD-A; JAAD",metrics: "KLDiv, CC for fixation-based, F-score, MAE for segmentation based"},
{reference_link: "https://arxiv.org/pdf/2003.06045.pdf",reference: "<a href=\"https://arxiv.org/pdf/2003.06045.pdf\">2020_arXiv_Zhang</a>",title: "Interaction Graphs for Object Importance Estimation in On-road Driving Videos",venue: "arXiv",year: "2020",code: "-",input: "RGB",observation: "16 frames",output: "bbox + importance score",dataset: "2019_ICRA_Gao",metrics: "AP"},
{reference_link: "https://doi.org/10.1109/WACV.2019.00035",reference: "<a href=\"https://doi.org/10.1109/WACV.2019.00035\">2019_WACV_Tavakoli</a>",title: "Digging Deeper into Egocentric Gaze Prediction",venue: "WACV",year: "2019",code: "-",input: "RGB, OF",observation: "1 frame",output: "saliency map",dataset: "USC Video Games",metrics: "AUC, NSS"},
{reference_link: "https://doi.org/10.1109/IVS.2019.8814210",reference: "<a href=\"https://doi.org/10.1109/IVS.2019.8814210\">2019_IV_Rahimpour</a>",title: "Context Aware Road-user Importance Estimation (iCARE)",venue: "IV",year: "2019",code: "-",input: "RGB, EV",observation: "1 frame",output: "bbox + importance score, ego-vehicle path",dataset: "custom: on-road",metrics: "PR curve, F-score"},
{reference_link: "https://doi.org/10.1109/ITSC.2019.8917337",reference: "<a href=\"https://doi.org/10.1109/ITSC.2019.8917337\">2019_ITSC_Ning</a>",title: "An Efficient Model for Driving Focus of Attention Prediction using Deep Learning",venue: "ITSC",year: "2019",code: "-",input: "RGB, OF",observation: "1 frame",output: "saliency map",dataset: "DR(eye)VE",metrics: "CC, KLDiv, IG"},
{reference_link: "https://doi.org/10.1109/ICRA.2019.8793970",reference: "<a href=\"https://doi.org/10.1109/ICRA.2019.8793970\">2019_ICRA_Gao</a>",title: "Goal-oriented object importance estimation in on-road driving videos",venue: "ICRA",year: "2019",code: "-",input: "RGB, EV",observation: "30 frames, path",output: "bbox + importance score",dataset: "custom: on-road",metrics: "AP, mAP"},
{reference_link: "https://doi.org/10.1109/TITS.2017.2766216",reference: "<a href=\"https://doi.org/10.1109/TITS.2017.2766216\">2018_T-ITS_Deng</a>",title: "Learning to Boost Bottom-Up Fixation Prediction in Driving Environments via Random Forest",venue: "T-ITS",year: "2018",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "Deng \etal 2018",metrics: "AUC, ROC, NSS"},
{reference_link: "https://doi.org/10.1109/TPAMI.2018.2845370",reference: "<a href=\"https://doi.org/10.1109/TPAMI.2018.2845370\">2018_PAMI_Palazzi</a>",title: "Predicting the Driver’s Focus of Attention: the DR(eye)VE Project",venue: "PAMI",year: "2018",code: "-",input: "RGB, OF, SM",observation: "16 frames",output: "saliency map",dataset: "DR(eye)VE",metrics: "CC, KLDiv, IG"},
{reference_link: "https://doi.org/10.1109/ITSC.2018.8569438",reference: "<a href=\"https://doi.org/10.1109/ITSC.2018.8569438\">2018_ITSC_Tawari</a>",title: "Learning to Attend to Salient Targets in Driving Videos Using Fully Convolutional RNN",venue: "ITSC",year: "2018",code: "-",input: "RGB",observation: "2 frames",output: "saliency map",dataset: "custom: on-road",metrics: "PR curve, CC, mAP"},
{reference_link: "https://doi.org/10.1007/978-3-030-20873-8_42",reference: "<a href=\"https://doi.org/10.1007/978-3-030-20873-8_42\">2018_ACCV_Xia</a>",title: "Predicting Driver Attention in Critical Situations",venue: "ACCV",year: "2018",code: "-",input: "RGB",observation: "6 frames",output: "saliency map",dataset: "BDD-A",metrics: "KLDiv, CC"},
{reference_link: "https://doi.org/10.1109/WACV.2017.52",reference: "<a href=\"https://doi.org/10.1109/WACV.2017.52\">2017_WACV_Palmer</a>",title: "Predicting the Perceptual Demands of Urban Driving with Video Regression",venue: "WACV",year: "2017",code: "-",input: "RGB",observation: "16 frames",output: "complexity score",dataset: "custom: on-road",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1016/j.patcog.2016.08.029",reference: "<a href=\"https://doi.org/10.1016/j.patcog.2016.08.029\">2017_PR_Ohn-Bar</a>",title: "Are all objects equal? Deep spatio-temporal importance prediction in driving videos",venue: "Pattern Recognition",year: "2017",code: "-",input: "RGB, BB, EV",observation: "2-3 s",output: "BB + importance score",dataset: "KITTI",metrics: "PR curve, mAP"},
{reference_link: "https://doi.org/10.1109/IVS.2017.7995828",reference: "<a href=\"https://doi.org/10.1109/IVS.2017.7995828\">2017_IV_Tawari</a>",title: "A Computational Framework for Driver’s Visual Attention Using A Fully Convolutional Architecture",venue: "IV",year: "2017",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "DR(eye)VE",metrics: "CC"},
{reference_link: "https://doi.org/10.1109/IVS.2017.7995833",reference: "<a href=\"https://doi.org/10.1109/IVS.2017.7995833\">2017_IV_Palazzi</a>",title: "Learning Where to Attend Like a Human Driver",venue: "IV",year: "2017",code: "<a href=\"https://github.com/francescosolera/dreyeving\">https://github.com/francescosolera/dreyeving</a>",input: "RGB",observation: "16 frames",output: "saliency map",dataset: "DR(eye)VE",metrics: "CC, KLDiv"},
{reference_link: "https://doi.org/10.1109/TITS.2016.2535402",reference: "<a href=\"https://doi.org/10.1109/TITS.2016.2535402\">2016_T-ITS_Deng</a>",title: "Where Does the Driver Look? Top-Down-Based Saliency Detection in a Traffic Driving Environment",venue: "T-ITS",year: "2016",code: "<a href=\"https://github.com/taodeng/Top-down-based-traffic-driving-saliency-model\">https://github.com/taodeng/Top-down-based-traffic-driving-saliency-model</a>",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "custom: on-road",metrics: "AUC, ROC, NSS"},
{reference_link: "https://arxiv.org/pdf/1611.05418v1.pdf",reference: "<a href=\"https://arxiv.org/pdf/1611.05418v1.pdf\">2016_arXiv_Bojarski_1</a>",title: "VisualBackProp: visualizing CNNs for autonomous driving",venue: "arXiv",year: "2016",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "custom: on-road",metrics: "qualitative"},
{reference_link: "https://doi.org/10.1109/TSMC.2013.2279715",reference: "<a href=\"https://doi.org/10.1109/TSMC.2013.2279715\">2014_TransSysManCybernetics_Borji</a>",title: "What/Where to Look Next? Modeling Top-Down Visual Attention in Complex Interactive Environments",venue: "Transactions on Systems, Man, and Cybernetics Systems",year: "2014",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "USC Video Games",metrics: "AUC, NSS"},
{reference_link: "https://doi.org/10.1016/j.trf.2013.09.019",reference: "<a href=\"https://doi.org/10.1016/j.trf.2013.09.019\">2013_TR_Wortelen</a>",title: "Dynamic simulation and prediction of drivers’ attention distribution",venue: "Transportation Research Part F",year: "2013",code: "-",input: "SIM",observation: "10 s",output: "gaze AOI + duration",dataset: "custom: sim",metrics: "correlation, RMSE, qualitative"},
{reference_link: "https://doi.org/10.1098/rstb.2013.0044",reference: "<a href=\"https://doi.org/10.1098/rstb.2013.0044\">2013_RSTB_Johnson</a>",title: "Predicting human visuomotor behaviour in a driving task",venue: "Philosophical Transactions of the Royal Society: B",year: "2013",code: "<a href=\"https://github.com/EmbodiedCognition/driving-simulator\">https://github.com/EmbodiedCognition/driving-simulator</a>",input: "SIM",observation: "1 timestep",output: "gaze AOI + duration",dataset: "custom: sim",metrics: "KLDiv, qualitative"},
{reference_link: "https://doi.org/10.1109/CVPR.2012.6247710",reference: "<a href=\"https://doi.org/10.1109/CVPR.2012.6247710\">2012_CVPR_Borji</a>",title: "Probabilistic Learning of Task-Specific Visual Attention",venue: "CVPR",year: "2012",code: "<a href=\"http://ilab.usc.edu/borji/Resources.html\">http://ilab.usc.edu/borji/Resources.html</a>",input: "RGB, PG, EV",observation: "1 frame",output: "saliency map",dataset: "USC Video Games",metrics: "NSS, AUC"},
{reference_link: "http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf",reference: "<a href=\"http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf\">2011_BMVC_Borji</a>",title: "Computational Modeling of Top-down Visual Attention in Interactive Environments",venue: "BMVC",year: "2011",code: "-",input: "RGB",observation: "1 frame",output: "saliency map",dataset: "USC Video Games",metrics: "NSS, AUC"},
	];

	var table = new Tabulator("#example-table", {
		height:700, // set height of table to enable virtual DOM
		data:tabledata, //load initial data into table
		layout:"fitColumns", //fit columns to width of table (optional)
		columns:[ //Define Table Columns
			{title:"Reference", field:"reference", sorter:"string", formatter:"html"},
			{title:"Title", field:"title", sorter:"string"},
			{title:"Venue", field:"venue", sorter:"string"},
			{title:"Year", field:"year", sorter:"number"},
			{title:"Code", field:"code", sorter:"string", formatter:"html"},
			{title:"Input", field:"input", sorter: "string"},
			{title:"Observation<br>length", field:"observation", sorter: "string"},
			{title:"Output", field:"output", sorter:"string"},
			{title:"Dataset", field:"dataset", sorter:"string"},
			{title:"Metrics", field:"metrics", sorter: "string"},
			{title:"Pipeline", field:"pipeline", sorter:"number"}
		],
	});
</script>
	<div>
		<p>Abbreviations</p>
		 <ul>
		  <li>RGB - 3-channel image</li>
		  <li>EV - ego-vehicle information</li>
		  <li>SIM - simulator data</li>
		  <li>PG - previous gaze location</li>
		  <li>OF - optical flow</li>
		  <li>SM - segmentation map</li>
		  <li>BB - bounding box</li>
		</ul> 
	</div>
	</body>
</html>