<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="assets/css/tabulator/4.9/tabulator.min.css" rel="stylesheet">
		<link href="assets/css/tables.css" rel="stylesheet">
	</head>


	<body>

 <!-- Top navigation -->
<div class="topnav">

<a href="https://github.com/ykotseruba/attention_and_driving">Back to Github</a>
<a href="behavioral_studies.html">Behavioral</a>
<a href="#" class="active">Practical</a>
<a href="datasets_datasets.html">Datasets</a>
<a href="surveys_surveys.html">Surveys</a>


</div> 

<div class="topnav">

<a href="#" class="active">Scene gaze</a>
<a href="practical_in_vehicle_gaze.html">In-vehicle gaze</a>
<a href="practical_distraction_detection.html">Distraction detection</a>
<a href="practical_drowsiness_detection.html">Drowsiness detection</a>
<a href="practical_action_anticipation.html">Action anticipation</a>
<a href="practical_driver_awareness.html">Driver awareness</a>
<a href="practical_self_driving.html">Self-driving</a>
<a href="practical_factors.html">Factors</a>


</div> 

		<div id="example-table"></div>
						<script type="text/javascript" src="assets/js/tabulator/4.9/tabulator.min.js"></script>
		<script type="text/javascript">
	//sample data
	var tabledata = [
	{reference_link: "https://openaccess.thecvf.com/content/ICCV2021/papers/Baee_MEDIRL_Predicting_the_Visual_Attention_of_Drivers_via_Maximum_Entropy_ICCV_2021_paper.pdf",bibtex: "<a href=\"all_bib.html#2021_ICCV_Baee\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Baee_MEDIRL_Predicting_the_Visual_Attention_of_Drivers_via_Maximum_Entropy_ICCV_2021_paper.pdf\">2021_ICCV_Baee</a>",title: "MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning",venue: "ICCV",year: "2021",code: "<a href=\"https://github.com/soniabaee/MEDIRL-EyeCar\">https://github.com/soniabaee/MEDIRL-EyeCar</a>",input: "S<sup>RGB</sup>",observation: "6 frames",processing: "DL",output: "saliency map",dataset: "public: Eyecar",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "CC, AUC, KLD"},
{reference_link: "https://doi.org/10.1109/TITS.2021.3055120",bibtex: "<a href=\"all_bib.html#2021_T-ITS_Amadori\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2021.3055120\">2021_T-ITS_Amadori</a>",title: "HammerDrive: A Task-Aware Driving Visual Attention Model",venue: "T-ITS",year: "2021",code: "-",input: "S<sup>RGB</sup>",observation: "16 frames",processing: "DL",output: "saliency map",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "simulation",metrics: "KLD, CC, SIM, IG"},
{reference_link: "https://doi.org/10.1109/TITS.2020.3044678",bibtex: "<a href=\"all_bib.html#2021_T-ITS_Fang\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2020.3044678\">2021_T-ITS_Fang</a>",title: "DADA: Driver Attention Prediction in Driving Accident Scenarios",venue: "T-ITS",year: "2021",code: "<a href=\"https://github.com/JWFangit/LOTVS-DADA\">https://github.com/JWFangit/LOTVS-DADA</a>",input: "S<sup>RGB</sup>",observation: "5 frames",processing: "DL",output: "saliency map",dataset: "public: TrafficSaliency, DReyeVE, DADA-2000",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "KLD, NSS, SIM, CC, AUC-J, AUC-S"},
{reference_link: "https://doi.org/10.1109/TITS.2019.2915540",bibtex: "<a href=\"all_bib.html#2020_T-ITS_Deng\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2019.2915540\">2020_T-ITS_Deng</a>",title: "How Do Drivers Allocate Their Potential Attention? Driving Fixation Prediction via Convolutional Neural Networks",venue: "T-ITS",year: "2020",code: "<a href=\"https://github.com/taodeng/CDNN-traffic-saliency\">https://github.com/taodeng/CDNN-traffic-saliency</a>",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "DL",output: "saliency map",dataset: "public: TrafficSaliency",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "AUC, NSS, IG, CC, SIM, EMD, KLD"},
{reference_link: "https://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_CVPR_Pal\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf\">2020_CVPR_Pal</a>",title: "“Looking at the right stuff” - Guided semantic-gaze for autonomous driving",venue: "CVPR",year: "2020",code: "<a href=\"https://sites.google.com/eng.ucsd.edu/sage-net\">https://sites.google.com/eng.ucsd.edu/sage-net</a>",input: "S<sup>RGB</sup>",observation: "16 frames",processing: "DL",output: "saliency map",dataset: "public: DR(eye)VE, BDD-A, JAAD",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "KLD, CC, F-score, MAE"},
{reference_link: "https://arxiv.org/pdf/2003.06045.pdf",bibtex: "<a href=\"all_bib.html#2020_ICRA_Zhang\">bib</a>",reference: "<a href=\"https://arxiv.org/pdf/2003.06045.pdf\">2020_ICRA_Zhang</a>",title: "Interaction Graphs for Object Importance Estimation in On-road Driving Videos",venue: "ICRA",year: "2020",code: "-",input: "S<sup>RGB</sup>",observation: "16 frames",processing: "DL",output: "BB + importance score",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "AP"},
{reference_link: "https://doi.org/10.1109/WACV.2019.00035",bibtex: "<a href=\"all_bib.html#2019_WACV_Tavakoli\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/WACV.2019.00035\">2019_WACV_Tavakoli</a>",title: "Digging Deeper into Egocentric Gaze Prediction",venue: "WACV",year: "2019",code: "-",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "DL",output: "saliency map",dataset: "public: 3DDS",attention_recording_conditions: "simulation",driving_recording_conditions: "simulation",metrics: "AUC, NSS"},
{reference_link: "https://doi.org/10.1109/IVS.2019.8814210",bibtex: "<a href=\"all_bib.html#2019_IV_Rahimpour\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IVS.2019.8814210\">2019_IV_Rahimpour</a>",title: "Context Aware Road-user Importance Estimation (iCARE)",venue: "IV",year: "2019",code: "-",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "DL",output: "BB + importance score, ego-vehicle path",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "PR curve, F-score"},
{reference_link: "https://doi.org/10.1109/ITSC.2019.8917337",bibtex: "<a href=\"all_bib.html#2019_ITSC_Ning\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ITSC.2019.8917337\">2019_ITSC_Ning</a>",title: "An Efficient Model for Driving Focus of Attention Prediction using Deep Learning",venue: "ITSC",year: "2019",code: "-",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "DL",output: "saliency map",dataset: "public: DR(eye)VE",attention_recording_conditions: "on-road",driving_recording_conditions: "on-road",metrics: "CC, KLD, IG"},
{reference_link: "https://doi.org/10.1109/ICRA.2019.8793970",bibtex: "<a href=\"all_bib.html#2019_ICRA_Gao\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ICRA.2019.8793970\">2019_ICRA_Gao</a>",title: "Goal-oriented object importance estimation in on-road driving videos",venue: "ICRA",year: "2019",code: "-",input: "S<sup>RGB</sup>",observation: "30 frames, path",processing: "DL",output: "BB + importance score",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "AP, mAP"},
{reference_link: "https://bmvc2019.org/wp-content/uploads/papers/0996-paper.pdf",bibtex: "<a href=\"all_bib.html#2019_BMVC_Palasek\">bib</a>",reference: "<a href=\"https://bmvc2019.org/wp-content/uploads/papers/0996-paper.pdf\">2019_BMVC_Palasek</a>",title: "Attentional demand estimation with attentive driving models",venue: "BMVC",year: "2019",code: "-",input: "S<sup>RGB</sup>",observation: "12 frames",processing: "DL",output: "visual demand",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "CC"},
{reference_link: "https://doi.org/10.1109/TITS.2017.2766216",bibtex: "<a href=\"all_bib.html#2018_T-ITS_Deng\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2017.2766216\">2018_T-ITS_Deng</a>",title: "Learning to Boost Bottom-Up Fixation Prediction in Driving Environments via Random Forest",venue: "T-ITS",year: "2018",code: "-",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "ML",output: "saliency map",dataset: "public: TETD",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "AUC, ROC, NSS"},
{reference_link: "https://doi.org/10.1109/TPAMI.2018.2845370",bibtex: "<a href=\"all_bib.html#2018_PAMI_Palazzi\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TPAMI.2018.2845370\">2018_PAMI_Palazzi</a>",title: "Predicting the Driver’s Focus of Attention: the DR(eye)VE Project",venue: "PAMI",year: "2018",code: "<a href=\"https://github.com/ndrplz/dreyeve\">https://github.com/ndrplz/dreyeve</a>",input: "S<sup>RGB</sup>",observation: "16 frames",processing: "DL",output: "saliency map",dataset: "public: DR(eye)VE",attention_recording_conditions: "on-road",driving_recording_conditions: "on-road",metrics: "CC, KLD, IG"},
{reference_link: "https://doi.org/10.1109/ITSC.2018.8569438",bibtex: "<a href=\"all_bib.html#2018_ITSC_Tawari\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ITSC.2018.8569438\">2018_ITSC_Tawari</a>",title: "Learning to Attend to Salient Targets in Driving Videos Using Fully Convolutional RNN",venue: "ITSC",year: "2018",code: "-",input: "S<sup>RGB</sup>",observation: "2 frames",processing: "DL",output: "saliency map",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "PR curve, CC, mAP"},
{reference_link: "https://doi.org/10.1007/978-3-030-20873-8_42",bibtex: "<a href=\"all_bib.html#2018_ACCV_Xia\">bib</a>",reference: "<a href=\"https://doi.org/10.1007/978-3-030-20873-8_42\">2018_ACCV_Xia</a>",title: "Predicting Driver Attention in Critical Situations",venue: "ACCV",year: "2018",code: "<a href=\"https://github.com/pascalxia/driver_attention_prediction\">https://github.com/pascalxia/driver_attention_prediction</a>",input: "S<sup>RGB</sup>",observation: "6 frames",processing: "DL",output: "saliency map",dataset: "public: BDD-A",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "KLD, CC"},
{reference_link: "https://doi.org/10.1109/WACV.2017.52",bibtex: "<a href=\"all_bib.html#2017_WACV_Palmer\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/WACV.2017.52\">2017_WACV_Palmer</a>",title: "Predicting the Perceptual Demands of Urban Driving with Video Regression",venue: "WACV",year: "2017",code: "-",input: "S<sup>RGB</sup>",observation: "16 frames",processing: "DL",output: "complexity score",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "Acc"},
{reference_link: "https://doi.org/10.1016/j.patcog.2016.08.029",bibtex: "<a href=\"all_bib.html#2017_PR_Ohn-Bar\">bib</a>",reference: "<a href=\"https://doi.org/10.1016/j.patcog.2016.08.029\">2017_PR_Ohn-Bar</a>",title: "Are all objects equal? Deep spatio-temporal importance prediction in driving videos",venue: "Pattern Recognition",year: "2017",code: "<a href=\"https://github.com/eshed1/Object_Importance\">https://github.com/eshed1/Object_Importance</a>",input: "S<sup>RGB</sup>",observation: "2-3 s",processing: "ML",output: "BB + importance score",dataset: "public: KITTI",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "PR curve, mAP"},
{reference_link: "https://doi.org/10.1109/IVS.2017.7995828",bibtex: "<a href=\"all_bib.html#2017_IV_Tawari\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IVS.2017.7995828\">2017_IV_Tawari</a>",title: "A Computational Framework for Driver’s Visual Attention Using A Fully Convolutional Architecture",venue: "IV",year: "2017",code: "-",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "DL",output: "saliency map",dataset: "public: DR(eye)VE",attention_recording_conditions: "on-road",driving_recording_conditions: "on-road",metrics: "CC"},
{reference_link: "https://doi.org/10.1109/IVS.2017.7995833",bibtex: "<a href=\"all_bib.html#2017_IV_Palazzi\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IVS.2017.7995833\">2017_IV_Palazzi</a>",title: "Learning Where to Attend Like a Human Driver",venue: "IV",year: "2017",code: "<a href=\"https://github.com/francescosolera/dreyeving\">https://github.com/francescosolera/dreyeving</a>",input: "S<sup>RGB</sup>",observation: "16 frames",processing: "DL",output: "saliency map",dataset: "public: DR(eye)VE",attention_recording_conditions: "on-road",driving_recording_conditions: "on-road",metrics: "CC, KLD"},
{reference_link: "https://doi.org/10.1109/TITS.2016.2535402",bibtex: "<a href=\"all_bib.html#2016_T-ITS_Deng\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2016.2535402\">2016_T-ITS_Deng</a>",title: "Where Does the Driver Look? Top-Down-Based Saliency Detection in a Traffic Driving Environment",venue: "T-ITS",year: "2016",code: "<a href=\"https://github.com/taodeng/Top-down-based-traffic-driving-saliency-model\">https://github.com/taodeng/Top-down-based-traffic-driving-saliency-model</a>",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "ML",output: "saliency map",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "on-road",metrics: "AUC, ROC, NSS"},
{reference_link: "https://doi.org/10.1109/TSMC.2013.2279715",bibtex: "<a href=\"all_bib.html#2014_TransSysManCybernetics_Borji\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TSMC.2013.2279715\">2014_TransSysManCybernetics_Borji</a>",title: "What/Where to Look Next? Modeling Top-Down Visual Attention in Complex Interactive Environments",venue: "Transactions on Systems, Man, and Cybernetics Systems",year: "2014",code: "-",input: "S<sup>RGB</sup>",observation: "1 frame",processing: "ML",output: "saliency map",dataset: "public: 3DDS",attention_recording_conditions: "simulation",driving_recording_conditions: "simulation",metrics: "AUC, NSS"},
{reference_link: "https://doi.org/10.1098/rstb.2013.0044",bibtex: "<a href=\"all_bib.html#2013_RSTB_Johnson\">bib</a>",reference: "<a href=\"https://doi.org/10.1098/rstb.2013.0044\">2013_RSTB_Johnson</a>",title: "Predicting human visuomotor behaviour in a driving task",venue: "Philosophical Transactions of the Royal Society: B",year: "2013",code: "<a href=\"https://github.com/EmbodiedCognition/driving-simulator\">https://github.com/EmbodiedCognition/driving-simulator</a>",input: "S<sup>SIM</sup>",observation: "1 timestep",processing: "ML",output: "gaze AOI + duration",dataset: "private",attention_recording_conditions: "simulation",driving_recording_conditions: "simulation",metrics: "KLD, qualitative"},
{reference_link: "https://doi.org/10.1109/CVPR.2012.6247710",bibtex: "<a href=\"all_bib.html#2012_CVPR_Borji\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/CVPR.2012.6247710\">2012_CVPR_Borji</a>",title: "Probabilistic Learning of Task-Specific Visual Attention",venue: "CVPR",year: "2012",code: "<a href=\"http://ilab.usc.edu/borji/Resources.html\">http://ilab.usc.edu/borji/Resources.html</a>",input: "RGB, PG, EV",observation: "1 frame",processing: "ML",output: "saliency map",dataset: "public: 3DDS",attention_recording_conditions: "simulation",driving_recording_conditions: "simulation",metrics: "NSS, AUC"},
{reference_link: "http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf",bibtex: "<a href=\"all_bib.html#2011_BMVC_Borji\">bib</a>",reference: "<a href=\"http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf\">2011_BMVC_Borji</a>",title: "Computational Modeling of Top-down Visual Attention in Interactive Environments",venue: "BMVC",year: "2011",code: "-",input: "RGB",observation: "1 frame",processing: "ML",output: "saliency map",dataset: "public: 3DDS",attention_recording_conditions: "simulation",driving_recording_conditions: "simulation",metrics: "NSS, AUC"},
	];

	var table = new Tabulator("#example-table", {
		height:700, // set height of table to enable virtual DOM
		data:tabledata, //load initial data into table
		layout:"fitColumns", //fit columns to width of table (optional)
		columns:[ //Define Table Columns
			{title:"Reference", field:"reference", sorter:"string", formatter:"html", width: 150},
			{title:"Bibtex", field:"bibtex", formatter:"html", width:30},
			{title:"Title", field:"title", sorter:"string", width: 150},
			{title:"Venue", field:"venue", sorter:"string", width: 90},
			{title:"Year", field:"year", sorter:"number", width: 50},
			{title:"Code", field:"code", sorter:"string", formatter:"html", width: 100},
			{title:"Input", field:"input", sorter: "string", formatter:"html", width:100},
			{title:"Observation<br>length", field:"observation", sorter: "string", width:100},
			{title:"Output", field:"output", sorter:"string"},
			{title:"Processing", field:"processing", sorter:"string", width: 120, align:"center"},
			{title:"Dataset", field:"dataset", sorter:"string"},
			{title:"Metrics", field:"metrics", sorter: "string"},
			{title:"Attention<br>recording<br>conditions", field:"attention_recording_conditions", sorter:"string"},
			{title:"Driving<br>recording<br>conditions", field:"driving_recording_conditions", sorter:"string"}

		],
	});
</script>
	<div>
		<p>Abbreviations</p>
		 <ul>
		  <li>S<sup>__</sup>, D<sup>__</sup> - scene/driver facing video</li>
		  <li><sup>RGB</sup> - 3-channel data</li>
		  <li><sup>SIM</sup> - simulator data</li>
		  <li>PG - previous gaze location</li>
		  <li>SM - segmentation map</li>
		  <li>BB - bounding box</li>
		</ul> 
	</div>
	</body>
</html>