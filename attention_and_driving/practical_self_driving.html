<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="assets/css/tabulator/4.9/tabulator.min.css" rel="stylesheet">
		<link href="assets/css/tables.css" rel="stylesheet">
	</head>


	<body>

 <!-- Top navigation -->
<div class="topnav">

<a href="https://github.com/ykotseruba/attention_and_driving">Back to Github</a>
<a href="behavioral_studies.html">Behavioral</a>
<a href="#" class="active">Practical</a>
<a href="datasets_datasets.html">Datasets</a>
<a href="surveys_surveys.html">Surveys</a>


</div> 

<div class="topnav">

<a href="practical_scene_gaze.html">Scene gaze</a>
<a href="practical_in_vehicle_gaze.html">In-vehicle gaze</a>
<a href="practical_inattention_detection.html">Inattention detection</a>
<a href="practical_action_anticipation.html">Action anticipation</a>
<a href="practical_driver_awareness.html">Driver awareness</a>
<a href="#" class="active">Self-driving</a>
<a href="practical_factors.html">Factors</a>


</div> 

		<div id="example-table"></div>
						<script type="text/javascript" src="assets/js/tabulator/4.9/tabulator.min.js"></script>
		<script type="text/javascript">
	//sample data
	var tabledata = [
	{reference_link: "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_CVPRW_Cultrera\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.pdf\">2020_CVPRW_Cultrera</a>",title: "Explaining Autonomous Driving by Learning End-to-End Visual Attention",venue: "CVPRW",year: "2020",code: "-",observation: "1 frame",input: "RGB from scene-facing camera",output: "steering angle",dataset: "Carla",metrics: "success rate, percentage of completed tasks"},
{reference_link: "https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_WACV_Xia\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf\">2020_WACV_Xia</a>",title: "Periphery-Fovea Multi-Resolution Driving Model Guided by Human Attention",venue: "WACV",year: "2020",code: "<a href=\"https://github.com/pascalxia/periphery_fovea_driving\">https://github.com/pascalxia/periphery_fovea_driving</a>",observation: "1 frame",input: "RGB from scene-facing camera",output: "speed",dataset: "BDD-X, BDD-A, DR(eye)VE",metrics: "MAE, RMSE, Corr"},
{reference_link: "https://doi.org/10.1109/IROS40897.2019.8967843",bibtex: "<a href=\"all_bib.html#2019_IROSW_Yuying\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IROS40897.2019.8967843\">2019_IROSW_Yuying</a>",title: "Gaze Training by Modulated Dropout Improves Imitation Learning",venue: "IROSW",year: "2019",code: "-",observation: "1 frame",input: "RGB from scene-facing camera",output: "steering angle",dataset: "TORCS",metrics: "distance between infractions"},
{reference_link: "https://doi.org/10.1109/ICRA.2019.8794224",bibtex: "<a href=\"all_bib.html#2019_ICRA_Wang\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ICRA.2019.8794224\">2019_ICRA_Wang</a>",title: "Deep Object-Centric Policies for Autonomous Driving",venue: "ICRA",year: "2019",code: "-",observation: "1 frame",input: "RGB from scene-facing camera",output: "steering angle, speed",dataset: "BDD",metrics: "interventions, distance between interventions, number of collisions"},
{reference_link: "https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf",bibtex: "<a href=\"all_bib.html#2019_CVPR_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf\">2019_CVPR_Kim</a>",title: "Grounding Human-to-Vehicle Advice for Self-driving Vehicles",venue: "CVPR",year: "2019",code: "-",observation: "20s",input: "RGB from scene-facing camera",output: "steering angle, speed",dataset: "HAD",metrics: "mean of correlation distances, MAE"},
{reference_link: "https://doi.org/10.1145/3314111.3319846",bibtex: "<a href=\"all_bib.html#2019_ACM_Liu\">bib</a>",reference: "<a href=\"https://doi.org/10.1145/3314111.3319846\">2019_ACM_Liu</a>",title: "A Gaze Model Improves Autonomous Driving",venue: "ACM Symposium on Eye Tracking Research and Applications",year: "2019",code: "-",observation: "1 frame",input: "RGB from scene-facing camera",output: "steering angle",dataset: "TORCS",metrics: "KLDiv, CC, MAE for steering angle"},
{reference_link: "https://doi.org/10.1109/ICPR.2018.8546051",bibtex: "<a href=\"all_bib.html#2018_ICPR_He\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ICPR.2018.8546051\">2018_ICPR_He</a>",title: "Aggregated Sparse Attention for Steering Angle Prediction",venue: "ICPR",year: "2018",code: "-",observation: "1 frame",input: "RGB from scene-facing camera",output: "steering angle",dataset: "DIPLECS, Comma.ai",metrics: "MAE"},
{reference_link: "https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf",bibtex: "<a href=\"all_bib.html#2018_ECCV_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf\">2018_ECCV_Kim</a>",title: "Textual Explanations for Self-Driving Vehicles",venue: "ECCV",year: "2018",code: "<a href=\"https://github.com/JinkyuKimUCB/explainable-deep-driving\">https://github.com/JinkyuKimUCB/explainable-deep-driving</a>",observation: "4 frames (0.4s)",input: "RGB from scene-facing camera",output: "steering angle, speed",dataset: "BDD-X",metrics: "MAE, CC"},
{reference_link: "https://openaccess.thecvf.com/content_ICCV_2017/papers/Kim_Interpretable_Learning_for_ICCV_2017_paper.pdf",bibtex: "<a href=\"all_bib.html#2017_ICCV_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_ICCV_2017/papers/Kim_Interpretable_Learning_for_ICCV_2017_paper.pdf\">2017_ICCV_Kim</a>",title: "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention",venue: "ICCV",year: "2017",code: "-",observation: "20 frames (1s)",input: "RGB from scene-facing camera",output: "steering angle",dataset: "Comma.ai, Udacity, custom: on-road",metrics: "MAE"},
{reference_link: "https://arxiv.org/pdf/1704.07911.pdf",bibtex: "<a href=\"all_bib.html#2017_arXiv_Bojarski\">bib</a>",reference: "<a href=\"https://arxiv.org/pdf/1704.07911.pdf\">2017_arXiv_Bojarski</a>",title: "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car",venue: "arXiv",year: "2017",code: "-",observation: "1 frame",input: "RGB from scene-facing camera",output: "steering angle",dataset: "custom in-vehicle",metrics: "qualitative, inverse R"},
	];

	var table = new Tabulator("#example-table", {
		height:700, // set height of table to enable virtual DOM
		data:tabledata, //load initial data into table
		layout:"fitColumns", //fit columns to width of table (optional)
		columns:[ //Define Table Columns
			{title:"Reference", field:"reference", sorter:"string", formatter:"html"},
			{title:"Bibtex", field:"bibtex", formatter:"html"},
			{title:"Title", field:"title", sorter:"string"},
			{title:"Venue", field:"venue", sorter:"string"},
			{title:"Year", field:"year", sorter:"number"},
			{title:"Code", field:"code", sorter:"string", formatter:"html"},
			{title:"Observation<br>length", field:"observation", sorter: "string"},
			{title:"Input", field:"input", sorter: "string"},
			{title:"Output", field:"output", sorter:"string"},
			{title:"Dataset", field:"dataset", sorter:"string"},
			{title:"Metrics", field:"metrics", sorter: "string"},
			{title:"Pipeline", field:"Pipeline", sorter:"number"}
		],
	});
</script>
	</body>
</html>