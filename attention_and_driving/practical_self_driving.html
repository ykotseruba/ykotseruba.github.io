<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="assets/css/tabulator/4.9/tabulator.min.css" rel="stylesheet">
		<link href="assets/css/tables.css" rel="stylesheet">
	</head>


	<body>

 <!-- Top navigation -->
<div class="topnav">

<a href="https://github.com/ykotseruba/attention_and_driving">Back to Github</a>
<a href="behavioral_studies.html">Behavioral</a>
<a href="#" class="active">Practical</a>
<a href="datasets_datasets.html">Datasets</a>
<a href="surveys_surveys.html">Surveys</a>


</div> 

<div class="topnav">

<a href="practical_scene_gaze.html">Scene gaze</a>
<a href="practical_in_vehicle_gaze.html">In-vehicle gaze</a>
<a href="practical_distraction_detection.html">Distraction detection</a>
<a href="practical_drowsiness_detection.html">Drowsiness detection</a>
<a href="practical_action_anticipation.html">Action anticipation</a>
<a href="practical_driver_awareness.html">Driver awareness</a>
<a href="#" class="active">Self-driving</a>
<a href="practical_factors.html">Factors</a>


</div> 

		<div id="example-table"></div>
						<script type="text/javascript" src="assets/js/tabulator/4.9/tabulator.min.js"></script>
		<script type="text/javascript">
	//sample data
	var tabledata = [
	{reference_link: "https://doi.org/10.1109/ICRA48506.2021.9561904",bibtex: "<a href=\"all_bib.html#2021_ICRA_Wei\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ICRA48506.2021.9561904\">2021_ICRA_Wei</a>",title: "Perceive, Attend, and Drive: Learning Spatial Attention for Safe Self-Driving",venue: "ICRA",year: "2021",code: "-",observation: "10 frames",input: " S<sup>MAP</sup>",output: "EV trajectory",dataset: "Drive4D, nuScenes \cite{2020_CVPR_Caesar}"},
{reference_link: "https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Ishihara_Multi-Task_Learning_With_Attention_for_End-to-End_Autonomous_Driving_CVPRW_2021_paper.pdf",bibtex: "<a href=\"all_bib.html#2021_CVPRW_Ishihara\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Ishihara_Multi-Task_Learning_With_Attention_for_End-to-End_Autonomous_Driving_CVPRW_2021_paper.pdf\">2021_CVPRW_Ishihara</a>",title: "Multi-task Learning with Attention for End-to-end Autonomous Driving",venue: "CVPRW",year: "2021",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle, throttle, brake",dataset: "CARLA (CoRL2017, NoCrash)"},
{reference_link: "https://openaccess.thecvf.com/content/ICCV2021/papers/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.pdf",bibtex: "<a href=\"all_bib.html#2021_ICCV_Chitta\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.pdf\">2021_ICCV_Chitta</a>",title: "NEAT: Neural Attention Fields for End-to-End Autonomous Driving",venue: "ICCV",year: "2021",code: "<a href=\"https://github.com/autonomousvision/neat\">https://github.com/autonomousvision/neat</a>",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle, throttle, brake",dataset: "CARLA"},
{reference_link: "https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf#page=1&zoom=auto,-100,798",bibtex: "<a href=\"all_bib.html#2021_CVPR_Prakash\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf#page=1&zoom=auto,-100,798\">2021_CVPR_Prakash</a>",title: "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",venue: "CVPR",year: "2021",code: "<a href=\"https://github.com/autonomousvision/transfuser\">https://github.com/autonomousvision/transfuser</a>",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle, throttle, brake",dataset: "CARLA"},
{reference_link: "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_CVPRW_Cultrera\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.pdf\">2020_CVPRW_Cultrera</a>",title: "Explaining Autonomous Driving by Learning End-to-End Visual Attention",venue: "CVPRW",year: "2020",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle",dataset: "CARLA"},
{reference_link: "https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_WACV_Xia\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf\">2020_WACV_Xia</a>",title: "Periphery-Fovea Multi-Resolution Driving Model Guided by Human Attention",venue: "WACV",year: "2020",code: "<a href=\"https://github.com/pascalxia/periphery_fovea_driving\">https://github.com/pascalxia/periphery_fovea_driving</a>",observation: "1 frame",input: "S<sup>RGB</sup>",output: "speed",dataset: "BDD-X, BDD-A, DR(eye)VE"},
{reference_link: "http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730273.pdf",bibtex: "<a href=\"all_bib.html#2020_ECCV_Zhou\">bib</a>",reference: "<a href=\"http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730273.pdf\">2020_ECCV_Zhou</a>",title: "DA4AD: End-to-end deep attention-based visual localization for autonomous driving",venue: "ECCV",year: "2020",code: "-",observation: "-",input: " S<sup>MAP</sup>",output: "-",dataset: "Apollo-DaoxiangLake"},
{reference_link: "https://doi.org/10.1109/IROS45743.2020.9341392",bibtex: "<a href=\"all_bib.html#2020_IROS_Li_1\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IROS45743.2020.9341392\">2020_IROS_Li_1</a>",title: "End-to-end Contextual Perception and Prediction with Interaction Transformer",venue: "IROS",year: "2020",code: "-",observation: "0.5s",input: "S<sup>RGB</sup>,  S<sup>MAP LiDAR</sup>",output: "EV trajectory",dataset: "ATG4D, nuScenes"},
{reference_link: "https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Advisable_Learning_for_Self-Driving_Vehicles_by_Internalizing_Observation-to-Action_Rules_CVPR_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_CVPR_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Advisable_Learning_for_Self-Driving_Vehicles_by_Internalizing_Observation-to-Action_Rules_CVPR_2020_paper.pdf\">2020_CVPR_Kim</a>",title: "Advisable Learning for Self-driving Vehicles by Internalizing Observation-to-Action Rules",venue: "CVPR",year: "2020",code: "<a href=\"https://github.com/JinkyuKimUCB/advisable-driving\">https://github.com/JinkyuKimUCB/advisable-driving</a>",observation: "20 frames",input: "S<sup>RGB</sup>",output: "speed, EV trajectory",dataset: "BDD-X, CARLA"},
{reference_link: "https://doi.org/10.1007/978-3-030-66096-3_6",bibtex: "<a href=\"all_bib.html#2020_ECCVW_Mittal\">bib</a>",reference: "<a href=\"https://doi.org/10.1007/978-3-030-66096-3_6\">2020_ECCVW_Mittal</a>",title: "AttnGrounder: Talking to Cars with Attention",venue: "ECCVW",year: "2020",code: "<a href=\"https://github.com/i-m-vivek/AttnGrounder\">https://github.com/i-m-vivek/AttnGrounder</a>",observation: "1 frame",input: "S<sup>RGB</sup>",output: "bbox",dataset: "Talk2Car"},
{reference_link: "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Kim_Attentional_Bottleneck_Towards_an_Interpretable_Deep_Driving_Network_CVPRW_2020_paper.pdf",bibtex: "<a href=\"all_bib.html#2020_CVPRW_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Kim_Attentional_Bottleneck_Towards_an_Interpretable_Deep_Driving_Network_CVPRW_2020_paper.pdf\">2020_CVPRW_Kim</a>",title: "Attentional Bottleneck: Towards an Interpretable Deep Driving Network",venue: "CVPRW",year: "2020",code: "-",observation: "-",input: "S<sup>MAP</sup>",output: "EV trajectory",dataset: "private: on-road"},
{reference_link: "https://doi.org/10.1109/IROS40897.2019.8967843",bibtex: "<a href=\"all_bib.html#2019_IROSW_Chen\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IROS40897.2019.8967843\">2019_IROSW_Chen</a>",title: "Gaze Training by Modulated Dropout Improves Imitation Learning",venue: "IROSW",year: "2019",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle",dataset: "TORCS"},
{reference_link: "https://doi.org/10.1109/ICRA.2019.8794224",bibtex: "<a href=\"all_bib.html#2019_ICRA_Wang\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ICRA.2019.8794224\">2019_ICRA_Wang</a>",title: "Deep Object-Centric Policies for Autonomous Driving",venue: "ICRA",year: "2019",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle, speed",dataset: "BDD"},
{reference_link: "https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf",bibtex: "<a href=\"all_bib.html#2019_CVPR_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf\">2019_CVPR_Kim</a>",title: "Grounding Human-to-Vehicle Advice for Self-driving Vehicles",venue: "CVPR",year: "2019",code: "-",observation: "20 frames",input: "S<sup>RGB</sup>",output: "steering angle, speed",dataset: "HAD"},
{reference_link: "https://doi.org/10.1145/3314111.3319846",bibtex: "<a href=\"all_bib.html#2019_ACM_Liu\">bib</a>",reference: "<a href=\"https://doi.org/10.1145/3314111.3319846\">2019_ACM_Liu</a>",title: "A Gaze Model Improves Autonomous Driving",venue: "ACM Symposium on Eye Tracking Research and Applications",year: "2019",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle",dataset: "TORCS"},
{reference_link: "https://doi.org/10.1109/IVS.2019.8813900",bibtex: "<a href=\"all_bib.html#2019_IV_Mori\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IVS.2019.8813900\">2019_IV_Mori</a>",title: "Visual Explanation by Attention Branch Network for End-to-end Learning-based Self-driving",venue: "IV",year: "2019",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle, throttle",dataset: "private: sim"},
{reference_link: "https://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Li_DBUS_Human_Driving_Behavior_Understanding_System_ICCVW_2019_paper.pdf",bibtex: "<a href=\"all_bib.html#2019_ICCVW_Li\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Li_DBUS_Human_Driving_Behavior_Understanding_System_ICCVW_2019_paper.pdf\">2019_ICCVW_Li</a>",title: "DBUS: Human Driving Behavior Understanding System",venue: "ICCVW",year: "2019",code: "-",observation: "25 frames",input: "S<sup>RGB</sup>",output: "maneuver, heatmap",dataset: "private: on-road"},
{reference_link: "https://doi.org/10.1109/ICPR.2018.8546051",bibtex: "<a href=\"all_bib.html#2018_ICPR_He\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ICPR.2018.8546051\">2018_ICPR_He</a>",title: "Aggregated Sparse Attention for Steering Angle Prediction",venue: "ICPR",year: "2018",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle",dataset: "DIPLECS, Comma.ai"},
{reference_link: "https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf",bibtex: "<a href=\"all_bib.html#2018_ECCV_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf\">2018_ECCV_Kim</a>",title: "Textual Explanations for Self-Driving Vehicles",venue: "ECCV",year: "2018",code: "<a href=\"https://github.com/JinkyuKimUCB/explainable-deep-driving\">https://github.com/JinkyuKimUCB/explainable-deep-driving</a>",observation: "4 frames (0.4s)",input: "S<sup>RGB</sup>",output: "steering angle, speed",dataset: "BDD-X"},
{reference_link: "https://doi.org/10.1109/ITSC.2018.8569464",bibtex: "<a href=\"all_bib.html#2018_ITSC_Mund\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ITSC.2018.8569464\">2018_ITSC_Mund</a>",title: "Visualizing the Learning Progress of Self-Driving Cars",venue: "ITSC",year: "2018",code: "-",observation: "1 frame",input: "S<sup>RGB</sup>",output: "heatmap",dataset: "https://github.com/SullyChen/driving-datasets"},
{reference_link: "https://openaccess.thecvf.com/content_ICCV_2017/papers/Kim_Interpretable_Learning_for_ICCV_2017_paper.pdf",bibtex: "<a href=\"all_bib.html#2017_ICCV_Kim\">bib</a>",reference: "<a href=\"https://openaccess.thecvf.com/content_ICCV_2017/papers/Kim_Interpretable_Learning_for_ICCV_2017_paper.pdf\">2017_ICCV_Kim</a>",title: "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention",venue: "ICCV",year: "2017",code: "-",observation: "20 frames (1s)",input: "S<sup>RGB</sup>",output: "steering angle",dataset: "Comma.ai, Udacity, private: on-road"},
{reference_link: "https://arxiv.org/pdf/1704.07911.pdf",bibtex: "<a href=\"all_bib.html#2017_arXiv_Bojarski\">bib</a>",reference: "<a href=\"https://arxiv.org/pdf/1704.07911.pdf\">2017_arXiv_Bojarski</a>",title: "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car",venue: "arXiv",year: "2017",code: "<a href=\"https://github.com/AutoDeep/PilotNet\">https://github.com/AutoDeep/PilotNet</a>",observation: "1 frame",input: "S<sup>RGB</sup>",output: "steering angle",dataset: "private: on-road"},
	];

	var table = new Tabulator("#example-table", {
		height:700, // set height of table to enable virtual DOM
		data:tabledata, //load initial data into table
		layout:"fitColumns", //fit columns to width of table (optional)
		columns:[ //Define Table Columns
			{title:"Reference", field:"reference", sorter:"string", formatter:"html"},
			{title:"Bibtex", field:"bibtex", formatter:"html", width:30},
			{title:"Title", field:"title", sorter:"string"},
			{title:"Venue", field:"venue", sorter:"string"},
			{title:"Year", field:"year", sorter:"number", width:50},
			{title:"Code", field:"code", sorter:"string", formatter:"html"},
			{title:"Observation<br>length", field:"observation", sorter: "string"},
			{title:"Input", field:"input", sorter: "string", formatter:"html"},
			{title:"Output", field:"output", sorter:"string"},
			{title:"Dataset", field:"dataset", sorter:"string"}
		],
	});
</script>
	</body>
</html>