<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="assets/css/tabulator/4.9/tabulator.min.css" rel="stylesheet">
		<link href="assets/css/tables.css" rel="stylesheet">
	</head>


	<body>

  <!-- Top navigation -->
<div class="topnav">

<a href="https://github.com/ykotseruba/attention_and_driving">Back to Github</a>
<a href="behavioral_studies.html">Behavioral</a>
<a href="#" class="active">Practical</a>
<a href="datasets_datasets.html">Datasets</a>
<a href="surveys_surveys.html">Surveys</a>


</div> 

<div class="topnav">

<a href="practical_scene_gaze.html">Scene gaze</a>
<a href="#" class="active">In-vehicle gaze</a>
<a href="practical_distraction_detection.html">Distraction detection</a>
<a href="practical_drowsiness_detection.html">Drowsiness detection</a>
<a href="practical_action_anticipation.html">Action anticipation</a>
<a href="practical_driver_awareness.html">Driver awareness</a>
<a href="practical_self_driving.html">Self-driving</a>
<a href="practical_factors.html">Factors</a>


</div> 

		<div id="example-table"></div>
						<script type="text/javascript" src="assets/js/tabulator/4.9/tabulator.min.js"></script>
		<script type="text/javascript">
	//sample data
	var tabledata = [
	{reference_link: "https://doi.org/10.1109/TITS.2021.3080322",bibtex: "<a href=\"all_bib.html#2021_T-ITS_Huang\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2021.3080322\">2021_T-ITS_Huang</a>",title: "Driver Glance Behavior Modeling based on Semi-supervised Clustering and Piecewise Aggregate Representation",venue: "T-ITS",year: "2021",code: "-",input: "glance data",observation: "-",output: "AOI",aois: "5",dataset: "private",recording_conditions: "simulator",location: "highway",properties: "gender, age, driving experience",metrics: "accuracy, P, R, F1, confusion matrix"},
{reference_link: "https://doi.org/10.1109/IV47402.2020.9304573",bibtex: "<a href=\"all_bib.html#2020_IV_Rangesh\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IV47402.2020.9304573\">2020_IV_Rangesh</a>",title: "Driver Gaze Estimation in the Real World: Overcoming the Eyeglass Challenge",venue: "IV",year: "2020",code: "<a href=\"https://github.com/arangesh/GPCycleGAN\">https://github.com/arangesh/GPCycleGAN</a>",input: "RGB, IR",observation: "1 frame",output: "AOI",aois: "6+EC",dataset: "public: LISA v3",recording_conditions: "on-road",location: "-",properties: "time of day, eyewear",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1145/3382507.3417967",bibtex: "<a href=\"all_bib.html#2020_ICMI_Stappen\">bib</a>",reference: "<a href=\"https://doi.org/10.1145/3382507.3417967\">2020_ICMI_Stappen</a>",title: "X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild",venue: "ICMI",year: "2020",code: "<a href=\"https://github.com/lstappen/XAWARE\">https://github.com/lstappen/XAWARE</a>",input: "RGB",observation: "1 frame",output: "AOI",aois: "9",dataset: "public: DGW",recording_conditions: "on-road",location: "parked vehicle",properties: "time of day, eyewear",metrics: "accuracy, F1, unweighted average recall (UAR)"},
{reference_link: "https://doi.org/10.1177/0018720820927687",bibtex: "<a href=\"all_bib.html#2020_HumanFactors_Jokinen\">bib</a>",reference: "<a href=\"https://doi.org/10.1177/0018720820927687\">2020_HumanFactors_Jokinen</a>",title: "Multitasking in Driving as Optimal Adaptation Under Uncertainty",venue: "Human Factors",year: "2020",code: "<a href=\"https://gitlab.com/jokinenj/multitasking-driving\">https://gitlab.com/jokinenj/multitasking-driving</a>",input: "gaze direction, SIM",observation: "1 timestep",output: "AOI",aois: "2",dataset: "private",recording_conditions: "on-road",location: "highway",properties: "-",metrics: "accuracy, qualitative"},
{reference_link: "https://doi.org/10.1109/TIV.2018.2843120",bibtex: "<a href=\"all_bib.html#2018_TIV_Vora\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TIV.2018.2843120\">2018_TIV_Vora</a>",title: "Driver Gaze Zone Estimation using Convolutional Neural Networks: A General Framework and Ablative Analysis",venue: "T-ITS",year: "2019",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "6+EC",dataset: "private",recording_conditions: "on-road",location: "-",properties: "gender, age, weather, time of day",metrics: "accuracy, confusion matrix"},
{reference_link: "https://doi.org/10.1109/TIV.2018.2804160",bibtex: "<a href=\"all_bib.html#2018_TIV_Martin\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TIV.2018.2804160\">2018_TIV_Martin</a>",title: "Dynamics of Driver’s Gaze: Explorations in Behavior Modeling & Maneuver Prediction",venue: "T-ITS",year: "2018",code: "-",input: "RGB",observation: "5s",output: "AOI",aois: "8+EC",dataset: "private",recording_conditions: "on-road",location: "highway, urban",properties: "-",metrics: "confusion matrix, accuracy, TP, FP"},
{reference_link: "https://doi.org/10.1109/IVS.2017.7995822",bibtex: "<a href=\"all_bib.html#2017_IV_Vora\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IVS.2017.7995822\">2017_IV_Vora</a>",title: "On Generalizing Driver Gaze Zone Estimation using Convolutional Neural Networks",venue: "IV",year: "2017",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "6+EC",dataset: "private",recording_conditions: "on-road",location: "-",properties: "-",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1109/TITS.2016.2526050",bibtex: "<a href=\"all_bib.html#2016_T-ITS_Lundgren\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2016.2526050\">2016_T-ITS_Lundgren</a>",title: "Driver-Gaze Zone Estimation Using Bayesian Filtering and Gaussian Processes",venue: "T-ITS",year: "2016",code: "-",input: "head pose, gaze direction, eye closure",observation: "-",output: "AOI",aois: "2, 5+EC",dataset: "private",recording_conditions: "on-road",location: "parked vehicle",properties: "-",metrics: "ROC curve, TP rate"},
{reference_link: "https://doi.org/10.1109/ITSC.2016.7795623",bibtex: "<a href=\"all_bib.html#2016_ITSC_Vasli\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ITSC.2016.7795623\">2016_ITSC_Vasli</a>",title: "On Driver Gaze Estimation: Explorations and Fusion of Geometric and Data Driven Approaches",venue: "ITSC",year: "2016",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "6",dataset: "private",recording_conditions: "on-road",location: "highway, urban",properties: "-",metrics: "accuracy, confusion matrix"},
{reference_link: "https://doi.org/10.1109/MIS.2016.47",bibtex: "<a href=\"all_bib.html#2016_IS_Fridman\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/MIS.2016.47\">2016_IS_Fridman</a>",title: "Driver Gaze Region Estimation without Use of Eye Movement",venue: "Pattern Recognition",year: "2016",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "2,6",dataset: "private",recording_conditions: "on-road",location: "highway",properties: "-",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1049/iet-cvi.2015.0296",bibtex: "<a href=\"all_bib.html#2016_IET_Fridman\">bib</a>",reference: "<a href=\"https://doi.org/10.1049/iet-cvi.2015.0296\">2016_IET_Fridman</a>",title: "‘Owl’ and ‘Lizard’: patterns of head pose and eye pose in driver gaze classification",venue: "IET Computer Vision",year: "2016",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "11",dataset: "private",recording_conditions: "on-road",location: "highway",properties: "-",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1109/BIGCOMP.2016.7425813",bibtex: "<a href=\"all_bib.html#2016_BigComp_Choi\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/BIGCOMP.2016.7425813\">2016_BigComp_Choi</a>",title: "Real-time categorization of driver’s gaze zone using the deep learning techniques",venue: "International Conference on Big Data and Smart Computing",year: "2016",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "8+EC",dataset: "private",recording_conditions: "on-road",location: "-",properties: "lighting, driving experience",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1109/TITS.2015.2396031",bibtex: "<a href=\"all_bib.html#2015_T-ITS_Vicente\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2015.2396031\">2015_T-ITS_Vicente</a>",title: "Driver Gaze Tracking and Eyes Off the Road Detection System",venue: "T-ITS",year: "2015",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "18",dataset: "private",recording_conditions: "on-road",location: "parked vehicle",properties: "time of day, eyewear, race, facial expressions",metrics: "accuracy, FP"},
{reference_link: "https://doi.org/10.1109/IVS.2014.6856607",bibtex: "<a href=\"all_bib.html#2014_IV_Tawari\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/IVS.2014.6856607\">2014_IV_Tawari</a>",title: "Robust and Continuous Estimation of Driver Gaze Zone by Dynamic Analysis of Multiple Face Videos",venue: "IV",year: "2014",code: "-",input: "RGB",observation: "2s",output: "AOI",aois: "8",dataset: "private",recording_conditions: "on-road",location: "-",properties: "-",metrics: "accuracy, confusion matrix"},
{reference_link: "https://doi.org/10.1109/ITSC.2014.6957817",bibtex: "<a href=\"all_bib.html#2014_ITSC_Tawari\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/ITSC.2014.6957817\">2014_ITSC_Tawari</a>",title: "Where is the driver looking: Analysis of Head, Eye and Iris for Robust Gaze Zone Estimation",venue: "ITSC",year: "2014",code: "-",input: "RGB",observation: "1 frame",output: "AOI",aois: "6",dataset: "private",recording_conditions: "on-road",location: "-",properties: "-",metrics: "accuracy, confusion matrix"},
{reference_link: "https://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W03/papers/Chuang_Estimating_Gaze_Direction_2014_CVPR_paper.pdf",bibtex: "<a href=\"all_bib.html#2014_CVPRW_Chuang\">bib</a>",reference: "<a href=\"https://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W03/papers/Chuang_Estimating_Gaze_Direction_2014_CVPR_paper.pdf\">2014_CVPRW_Chuang</a>",title: "Estimating Gaze Direction of Vehicle Drivers using a Smartphone Camera",venue: "CVPRW",year: "2014",code: "-",input: "RGB",observation: "5 frames",output: "AOI",aois: "8",dataset: "private",recording_conditions: "on-road",location: "parked vehicle",properties: "-",metrics: "accuracy"},
{reference_link: "https://doi.org/10.1109/TITS.2010.2091503",bibtex: "<a href=\"all_bib.html#2011_T-ITS_Lee\">bib</a>",reference: "<a href=\"https://doi.org/10.1109/TITS.2010.2091503\">2011_T-ITS_Lee</a>",title: "Real-Time Gaze Estimator Based on Driver’s Head Orientation for Forward Collision Warning System",venue: "T-ITS",year: "2011",code: "-",input: "IR",observation: "1 frame",output: "AOI",aois: "18",dataset: "private",recording_conditions: "on-road",location: "-",properties: "time of day, eyewear",metrics: "accuracy"},
	];

	var table = new Tabulator("#example-table", {
		height:700, // set height of table to enable virtual DOM
		data:tabledata, //load initial data into table
		layout:"fitColumns", //fit columns to width of table (optional)
		columns:[ //Define Table Columns
			{title:"Reference", field:"reference", sorter:"string", formatter:"html", width:150},
			{title:"Bibtex", field:"bibtex", formatter:"html", width:30},
			{title:"Title", field:"title", sorter:"string", width:150},
			{title:"Venue", field:"venue", sorter:"string", width: 90},
			{title:"Year", field:"year", sorter:"number", width: 50},
			{title:"Input", field:"input", sorter: "string", formatter:"html", width: 100},
			{title:"Observation<br>length", field:"observation", sorter: "string", width: 100},
			{title:"Output", field:"output", sorter:"string"},
			{title:"# AOIs", field:"aois", sorter:"string"},
			{title:"Dataset", field:"dataset", sorter:"string"},
			{title:"Recording<br>conditions", field:"recording_conditions", sorter:"string"},
			{title:"Location", field:"location", sorter:"string"},
			{title:"Properties", field:"properties", sorter:"string"},
			{title:"Metrics", field:"metrics", sorter: "string"}
		],
	});
</script>
	<div>
		<p>Abbreviations</p>
		 <ul>
		  <li>RGB - 3-channel image from driver-facing camera</li>
		  <li>IR - infrared image from driver-facing camera</li>
		  <li>EC - eyes closed</li>
		  <li>AOI - area of interest</li>
		  <li>SCER - ratio of number of strictly correct frames to total frames</li>
		  <li>LCER - ratio of loosely correct frames to total frames</li>
		</ul> 
	</div>
	</body>
</html>